---
title: "Assignment_8"
author: "Clustered Errors"
date: "3/14/2018"
output:
  ioslides_presentation: 
    widescreen: true 
    smaller: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(MASS)
library(broom)
library(modelr)
library(tidyverse)
select <- dplyr::select
```

```{r sim}
sim_reg <- function(n=100, B=200) {
  Sigma <-matrix(c(1, 0.7, 0.7, 1), 2)
  X <-mvrnorm(n=n, c(0, 0), Sigma = Sigma)
  y <- 1.2 + 0.3*X[,1]+1.1*X[,2] + rnorm(n)
  df <-data_frame(y=y, X1=X[,1], X2=X[,2])
  
  ## reg
  reg <-lm(y~X1+X2, data=df)
  reg_su <-summary(reg)
  reg_co <-tidy(reg_su)
  co_X0 <- reg_co$estimate[1]
  co_X1 <- reg_co$estimate[2]
  co_X2 <- reg_co$estimate[3]
  
  theta_hat <- 1.1*co_X1 - 0.3*co_X2
  
  ## Delta
  # R <- COMPUTE IT for the three parameters
  R <- matrix(c(0, 1.1, -0.3),3)
  v_theta <- as.numeric(t(R) %*% vcov(reg_su) %*% R)

  
  # W-test
  W_test <- ((theta_hat)^2)/v_theta
  
  ## boot it:
  x_boot <- modelr::bootstrap(df, n=B) %>% 
    mutate(reg = map(strap, ~lm(y~X1+X2, data=.)),
           reg_su = map(reg, summary),
           reg_co = map(reg_su, tidy),
           theta_b = map_dbl(reg_co, ~ 1.1 * .$estimate[2] - 0.3 * .$estimate[3]),
           vcov_b = map_dbl(reg_su, ~ t(R) %*% vcov(.) %*% R),
           W_test_b = map2_dbl(theta_b, vcov_b, ~ ((.x)^2)/.y),
           W_test_b2 = map2_dbl(theta_b, vcov_b, ~ ((.x - theta_hat)^2)/.y))
  x_boot2 <- x_boot %>% 
    summarise(boot_sd = sd(theta_b),
              wtest_cval = quantile(W_test_b, probs = c(0.95)),
              wtest2_cval = quantile(W_test_b2, probs = c(0.95)),
              W_test_b2 = mean(W_test_b2),
              W_test_b = mean(W_test_b),
              boot_sd_v = mean(vcov_b),
              boot_mean = mean(theta_b))
  
  ## results: original Wald and va, and boot aggregates
  data_frame(co_X1 = co_X1, co_X2 = co_X2, theta = theta_hat, W_test = W_test, var = v_theta) %>% 
    bind_cols(x_boot2)
  }
```

```{r}
simulations <- bind_rows(rerun(200, sim_reg()))
simulations <- mutate(simulations, reject_3 = (simulations$W_test >= 3.84), reject_4 = (simulations$W_test_b >=3.84), reject_5 = (simulations$W_test >= simulations$wtest_cval), reject_5_1 = (simulations$W_test >= simulations$wtest2_cval))
arr_3 <- mean(simulations$reject_3)
arr_4 <-mean(simulations$reject_4)
arr_5 <- mean(simulations$reject_5)
arr_5_1 <- mean(simulations$reject_5_1)

```

## Comments

We find that the average rejection rate over the bootstrap samples is 0 when using the bootstrap critical value using the hypothesis centered around the true value. This makes sense as the bootstrap critical value becomes very large.

We used the linear test from the beginning. By doing this we gained efficiency by not needing to approximate the function using the delta method. 


# Cross Validation

```{r cross_validation}
# Generate Data
n <- 100
S <- matrix(c(1, 0.5, 0.5, 1), 2)
set.seed(2018)
X <- mvrnorm(n = n, mu = c(0, 0), Sigma = S)
y <- 1.2 + 0.3 * X[, 1] + rnorm(n)

data_df <- data_frame(y = y, x1 = X[, 1], x2 = X[, 2])

# Perform Cross Validation
cv_df <- crossv_kfold(data_df, k = n)

crossval <- cv_df %>%
  mutate(reg1 = map(train, ~ lm(y ~ x1, data = .)),
         reg2 = map(train, ~ lm(y ~ x1 + x2, data = .))) %>%
  gather(model, reg, reg1, reg2) %>%
  mutate(resids_out = map2_dbl(reg, test, ~ predict(.x, newdata = .y) - as_data_frame(.y)$y),
         mse_in = map_dbl(reg, ~ mean(residuals(.)^2)),
         mse_out = map_dbl(resids_out, ~ mean(.^2))) %>%
  gather(mse, value, mse_in, mse_out)
```

## Cross Validation: The Crossval Data Frame

The Cross Validation data frame is a $100 \times 3$ data frame, with 100 rows (corresponding to the number of observations in the sample) and three variables. The first variable is the training set (the sample containing the 99 observations not removed) and the second is the testing set (the one observation removed). The objects contained in both of these variables are themselves data frames (though they are stored as lists to manage storage space). The lists in train and test are length 2, but the data frames contained within are also $100 \times 3$. 

## Cross Validation: Output Data Frame

The output data frame contains the results of the regressions and the calculations of mean squared error. The _MSE_ column contains character strings that refer to whether the mean squared error value contained in _value_ is from the in-sample estimation or out-of-sample estimation. Similarly, the _model_ variable simply is an indicator for whether the regression results are from the regression of $y$ on $x_1$ alone or from $y$ on $x_1$ and $x_2$. 

The variable `resids_out` is calculated by using the estimated regression model from the training set to predict values using the one observation left out of the sample (the testing set). Essentially, the `map_dbl` function takes values from the variables `reg` and `test`, which are are regression output and a data frame respectively, uses the function `predict` to predict $\hat{y}$ values, and calculates the residuals as $\hat{y} - y$. In this case, `.x` is the regression output stored in `reg`, and `.y` is the testing set in `test`. 

## Cross Validation: Summary of Results

```{r}
crossval_summary <- crossval %>%
  group_by(model, mse) %>%
  summarize(overall_mse = mean(value))

kable(crossval_summary, digits = 3, col.names = c("Model", "In vs. Out", "Overall MSE"))
```

_Reg1 vs. Reg2:_ Counterintuitely, we found that the mean squared error was slightly lower from the regression of $y$ on $x_1$ and $x_2$ as compared to the regression on $x_1$ alone. This is surprising given that the underlying DGP has $y$ as a function of only $x_1$. Therefore, the model with only $x_1$ should be expected to perform better. However, the difference in MSE between both models is very small. 

_In-Sample vs. Out-of-Sample:_ For both models the in-sample MSE is lower. This is to be expected given that the models are estimated using the data in the sample, so the MSE is bound to be lower when compared to the error from the out-of-sample prediction. 

## Cross Validation: Estimation on the Full Data Set

```{r}
full_reg <- lm(y ~ x1 + x2, data_df)
h <- hatvalues(full_reg)
resids <- residuals(full_reg)
sigma_tilde <- (resids^2) / ((1 - h)^2)
mean_sigma_tilde <- mean(sigma_tilde)

crossval_summary[5, ] <- c("hansen", "mse_out", mean_sigma_tilde)
kable(crossval_summary, digits = 3, col.names = c("Model", "In vs. Out", "Overall MSE"))
```

Calculating the out-of-sample MSE using Hansen equation 3.47 results in exactly the same out-of-sample MSE as from the simulation. 



